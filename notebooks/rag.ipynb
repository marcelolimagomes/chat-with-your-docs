{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "os.environ['allow_dangerous_deserialization'] = 'true'\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/*.txt\"\n",
    "faiss_database = \"../data/faiss_index\"\n",
    "file_list = glob.glob(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "  # 1️⃣ Carregar documentos (pode substituir pelo seu próprio dataset)\n",
    "  loader = TextLoader(file)  # Carregue um arquivo local\n",
    "  documents = loader.load()\n",
    "  \n",
    "  return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3️⃣ Criar embeddings com Ollama\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")  # Usa o modelo \"mistral\" para embeddings\n",
    "vectorstore = None\n",
    "new_vecstore = None\n",
    "if os.path.exists(faiss_database):\n",
    "  vectorstore = FAISS.load_local(faiss_database, embeddings, allow_dangerous_deserialization=True)\n",
    "  print(f'Vecstore loaded from {faiss_database}')\n",
    "  \n",
    "lines = []\n",
    "for file in file_list:\n",
    "  with open(file, 'r') as f:\n",
    "    lines.extend(f.readlines())\n",
    "  os.rename(file, file.replace('data', 'data/indexed_files'))\n",
    "\n",
    "if len(lines) > 0:    \n",
    "  with open('.temp', 'w') as f_out:\n",
    "    f_out.writelines(lines)  \n",
    "  # 1️⃣ Carregar documentos (pode substituir pelo seu próprio dataset)\n",
    "  loader = TextLoader(\".temp\")  # Carregue um arquivo local\n",
    "  documents = loader.load()\n",
    "\n",
    "  # 2️⃣ Dividir o texto em pedaços menores para indexação eficiente\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "  texts = text_splitter.split_documents(documents)\n",
    "\n",
    "  # 3️⃣ Criar embeddings com Ollama\n",
    "  embeddings = OllamaEmbeddings(model=\"mistral\")  # Usa o modelo \"mistral\" para embeddings\n",
    "\n",
    "  # 4️⃣ Criar e armazenar a base de dados vetorial com FAISS\n",
    "  new_vecstore = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "if (vectorstore is not None) and (new_vecstore is not None):\n",
    "  vectorstore.merge_from(new_vecstore)\n",
    "  vectorstore.save_local(faiss_database)\n",
    "  print(f'{faiss_database} Updated with new files!!!')\n",
    "elif new_vecstore is not None and vectorstore is None:\n",
    "  # Save FAISS index locally\n",
    "  new_vecstore.save_local(faiss_database)  \n",
    "  vectorstore = new_vecstore  \n",
    "  print(f'{faiss_database} Loaded!!!')\n",
    "elif (vectorstore is None) and (new_vecstore is None):\n",
    "  raise Exception('No previous vector db found and no new files do vectorize!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5️⃣ Carregar o modelo Llama 3.1 com Ollama para geração de texto\n",
    "#llm = OllamaLLM(model='llama3.1')  \n",
    "llm = OllamaLLM(model='llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6️⃣ Criar o sistema de Perguntas e Respostas (RAG)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Definir um template de prompt em português\n",
    "template = \"\"\"\n",
    "Responda em português, considere que estamos no país Brasil, a seguinte pergunta com base no contexto fornecido. Sempre informe a fonte da informação:\n",
    "\n",
    "Pergunta: {question}\n",
    "Contexto: {context}\n",
    "\n",
    "Resposta em português:\n",
    "\"\"\"\n",
    "\n",
    "# Criar o PromptTemplate\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "\n",
    "# Configurar o RetrievalQA com o prompt personalizado\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_PROMPT},  # Usar o prompt personalizado    \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "#query = \"Quais são os temas tratados nos documentos?\"\n",
    "#query = 'Quais os direitos relacionados a férias?'\n",
    "query = 'Qual a pena para cidadãos que cometem assassinato?'\n",
    "\n",
    "# Fazer uma pergunta\n",
    "response = qa_chain.invoke(query)\n",
    "#chain = ConversationChain(llm=llm)\n",
    "#response = chain.invoke(query)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
