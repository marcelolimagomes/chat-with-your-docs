{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['allow_dangerous_deserialization'] = 'true'\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_community.vectorstores import FAISS\n",
    "#from langchain_chroma import Chroma \n",
    "from langchain.chains import RetrievalQA, ConversationChain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "# importing required modules\n",
    "from pypdf import PdfReader\n",
    "\n",
    "import string\n",
    "import tiktoken\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_data_path = \"../data/documents/*.pdf\"\n",
    "txt_data_path = \"../data/documents/*.txt\"\n",
    "path_database = \"../data/database\"\n",
    "list_pdf_files = glob.glob(pdf_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "  while '  ' in text:\n",
    "    text = text.replace('  ', ' ')    \n",
    "    \n",
    "  for s in string.punctuation:\n",
    "    text = text.replace(s+s, s)\n",
    "    \n",
    "  return text.strip()\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "  if '.pdf' not in file_path:\n",
    "    raise Exception(f'File {file_path} is not a pdf file!')\n",
    "  \n",
    "  result = []\n",
    "  # creating a pdf reader object\n",
    "  reader = PdfReader(file_path)\n",
    "\n",
    "  # getting a specific page from the pdf file\n",
    "  for page in reader.pages:\n",
    "    # extracting text from page\n",
    "    text = page.extract_text()\n",
    "    text = clean_text(text)\n",
    "    if len(text.strip()) == 0:\n",
    "      continue\n",
    "    result.append(text)\n",
    "  return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pdf_file in list_pdf_files:\n",
    "  print(f'Extracting text from {pdf_file} ...')\n",
    "  lines = extract_text_from_pdf(pdf_file)\n",
    "  with open(pdf_file.replace('pdf', 'txt'), 'w') as f:\n",
    "    f.writelines(lines)\n",
    "  os.rename(pdf_file, pdf_file.replace('documents', 'indexed_files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Criar embeddings com Ollama\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")  # Usa o modelo \"mistral\" para embeddings\n",
    "\n",
    "vectorstore = None \n",
    "new_vectors = None \n",
    "\n",
    "if os.path.isfile(path_database + '/index.faiss'):\n",
    "  vectorstore = FAISS.load_local(path_database, embeddings, allow_dangerous_deserialization=True)  # Carrega o vetorstore salvo em disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîπ Load and process new documents\n",
    "new_documents = []\n",
    "list_txt_files = glob.glob(txt_data_path)\n",
    "for txt_file in list_txt_files:\n",
    "    loader = TextLoader(txt_file)\n",
    "    new_documents.extend(loader.load())\n",
    "    os.rename(txt_file, txt_file.replace('documents', 'indexed_files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of new documents: {len(new_documents)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(new_documents) > 0:\n",
    "  # 2Ô∏è‚É£ Dividir o texto em peda√ßos menores para indexa√ß√£o eficiente\n",
    "  text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "  texts = text_splitter.split_documents(new_documents)\n",
    "  # üîπ Generate embeddings\n",
    "  vectorstore = FAISS.from_documents(texts, embeddings)\n",
    "  vectorstore.save_local(path_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (vectorstore is not None) and (new_vectors is not None):\n",
    "  vectorstore.merge_from(new_vectors)\n",
    "  vectorstore.save_local(path_database)\n",
    "  print(f'{path_database} Updated with new files!!!')\n",
    "  \n",
    "elif new_vectors is not None and vectorstore is None:\n",
    "  # Save FAISS index locally\n",
    "  new_vectors.save_local(path_database)  \n",
    "  vectorstore = new_vectors  \n",
    "  print(f'{path_database} Loaded!!!')\n",
    "elif (vectorstore is None) and (new_vectors is None):\n",
    "  raise Exception('No previous vector db found and no new files do vectorize!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()  \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ Carregar o modelo Llama 3.1 com Ollama para gera√ß√£o de texto\n",
    "#llm = OllamaLLM(model='llama3.1')  \n",
    "llm = OllamaLLM(model='llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Voc√™ √© um assistente de modelo de linguagem de IA. Sua tarefa √© gerar cinco\n",
    "vers√µes diferentes da pergunta do usu√°rio fornecida para recuperar documentos relevantes de um vector database. \n",
    "Ao gerar m√∫ltiplas perspectivas sobre a pergunta do usu√°rio, seu objetivo √© ajudar\n",
    "o usu√°rio a superar algumas das limita√ß√µes da busca por similaridade baseada em dist√¢ncia.\n",
    "Forne√ßa essas perguntas alternativas separadas por quebras de linha. Pergunta original: {question}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = 'Quais s√£o os crimes ediondos citados no documento?'\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "#docs = retrieval_chain.invoke({\"question\":question})\n",
    "#len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "template = \"\"\"Responta a seguinte quest√£o baseada no contexto abaixo:\n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Quest√£o: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "template = \"\"\"Responda a seguinte quest√£o baseado neste contexto:\n",
    "\n",
    "{context}\n",
    "\n",
    "Quest√£o: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"Voc√™ √© um assistente √∫til que gera v√°rias subperguntas relacionadas a uma pergunta de entrada. \\n\n",
    "O objetivo √© dividir a entrada em um conjunto de subproblemas/subquest√µes que podem ser respondidos isoladamente. \\n\n",
    "Gere v√°rias consultas de pesquisa relacionadas a: {question} \\n\n",
    "Gere 3 consultas:\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "generate_queries_decomposition = (  prompt_decomposition \n",
    "                                  | llm \n",
    "                                  | StrOutputParser() \n",
    "                                  | (lambda x: x.split(\"\\n\")) )\n",
    "\n",
    "# Run\n",
    "question = \"Quais s√£o os princ√≠pios do c√≥digo penal brasileiro?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Aqui est√° a pergunta que voc√™ precisa responder:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Aqui est√£o quaisquer pares de perguntas e respostas de contexto dispon√≠veis:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Aqui est√° um contexto adicional relevante para a pergunta:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use o contexto acima e quaisquer pares de perguntas e respostas de contexto para responder √† pergunta: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Quest√£o: {question}\\nResposta: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "answers = []\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    answers.append(answer)\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair\n",
    "    \n",
    "print(q_a_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questions), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Pergunta {i}: {question}\\nResposta {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Aqui est√° um conjunto de pares de Perguntas e Respostas:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use este conjunto para sintetizar uma resposa para pergunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "resposta_final = final_rag_chain.invoke({\"context\":context,\"question\":question})\n",
    "\n",
    "print(resposta_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Quais s√£o os principios do C√≥digo Penal Brasileiro?\",\n",
    "        \"output\": \"Os princ√≠pios do C√≥digo Penal Brasileiro est√£o declarados nos documentos enviados?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"As for√ßas de seguran√ßa do Brasil s√£o prevista no c√≥digo penal?\",\n",
    "        \"output\": \"Quais s√£o as for√ßas de seguran√ßa?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"Voc√™ √© um especialista em conhecimento mundial. Sua tarefa √© recuar e parafrasear uma pergunta para uma pergunta de recuo mais gen√©rica, que √© mais f√°cil de responder. Aqui est√£o alguns exemplos:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "question = \"Diga o que est√° previsto nos documentos sobre c√≥digo penal?\"\n",
    "print(question)\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
